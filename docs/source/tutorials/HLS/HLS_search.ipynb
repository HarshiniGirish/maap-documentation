{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb8d36e",
   "metadata": {},
   "source": [
    "# Harmonized Landsat Sentinel (HLS) Search and Composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40ba39",
   "metadata": {},
   "source": [
    "In this tutorial we will search the LPDAAC for HLS 30m optical imagery that intersects an AOI. We will filter the catalog based on a cloud cover % and build a maximum-NDVI composite image, including a suite of popular indices. We will begin by installing any packages we need and importing the packages that we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd17893",
   "metadata": {},
   "source": [
    "If needed the following packages should be installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup data: removes data files that should be replaced\n",
    "!rm -rf ./local-s3.json\n",
    "!rm -rf ./sample.json\n",
    "\n",
    "if False:\n",
    "    !conda install -c conda-forge pystac-client -y\n",
    "    !conda install -c conda-forge rio-tiler -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123134a",
   "metadata": {},
   "source": [
    "We will now import a suite of packages that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8aa72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host='api.ops.maap-project.org')\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import h5py\n",
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from pystac_client import Client\n",
    "import datetime\n",
    "import os\n",
    "import rasterio as rio\n",
    "import boto3\n",
    "import json\n",
    "import botocore\n",
    "from rasterio.session import AWSSession\n",
    "from rio_tiler.io import COGReader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25576924",
   "metadata": {},
   "source": [
    "Creating an AOI: \n",
    "\n",
    "To begin we will create a polygon in a forested area in Virginia, USA. We will do this by providing a number of lat/lon coordinates and create an AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e43d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_coords = [-80, -80, -79., -79, -80]\n",
    "lat_coords = [39, 38.2, 38.2, 39, 39]\n",
    "\n",
    "polygon_geom = Polygon(zip(lon_coords, lat_coords))\n",
    "crs = 'epsg:4326'\n",
    "AOI = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])\n",
    "AOI_bbox = AOI.bounds.iloc[0].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941c1c1",
   "metadata": {},
   "source": [
    "We can visualize this polygon using a folium interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e289a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map([38.5,-79.3], zoom_start=9, tiles='OpenStreetMap')\n",
    "folium.GeoJson(AOI).add_to(m)\n",
    "folium.LatLngPopup().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b31440",
   "metadata": {},
   "source": [
    "Accessing the HLS Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb00ed3",
   "metadata": {},
   "source": [
    "To be able to access the HLS imagery we need to activate a 'rasterio' AWS session. This will give us the required access keys that we need to search and read data from the LPDAAC S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aws_session_DAAC():\n",
    "    \"\"\"Create a Rasterio AWS Session with Credentials\"\"\"\n",
    "    creds = maap.aws.earthdata_s3_credentials('https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials')\n",
    "    boto3_session = boto3.Session(\n",
    "        aws_access_key_id=creds['accessKeyId'], \n",
    "        aws_secret_access_key=creds['secretAccessKey'],\n",
    "        aws_session_token=creds['sessionToken'],\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "    return AWSSession(boto3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting AWS credentials...')\n",
    "aws_session = get_aws_session_DAAC()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3991c",
   "metadata": {},
   "source": [
    "Now that we have our session credentials set up, we can search the HLS catalog using the following function, filtering by spatial extent (our AOI) and a time window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_stac(year, bbox, max_cloud, api, start_month_day, end_month_day):\n",
    "    print('opening client')\n",
    "    catalog = Client.open(api)\n",
    "    \n",
    "    date_min = str(year) + '-' + start_month_day\n",
    "    print('start_month_day:\\t\\t', start_month_day)\n",
    "    print('end_month_day:\\t\\t', end_month_day)\n",
    "    date_max = str(year) + '-' + end_month_day\n",
    "    start_date = datetime.datetime.strptime(date_min, \"%Y-%m-%d\")\n",
    "    end_date = datetime.datetime.strptime(date_max, \"%Y-%m-%d\") \n",
    "    start = start_date.strftime(\"%Y-%m-%dT00:00:00Z\")\n",
    "    end = end_date.strftime(\"%Y-%m-%dT23:59:59Z\")\n",
    "    \n",
    "    print('start date, end date:\\t\\t', start, end)\n",
    "    \n",
    "    print('\\nConducting HLS search now...')\n",
    "    \n",
    "    search = catalog.search(\n",
    "        collections=[\"HLSL30.v2.0\"],\n",
    "        datetime=[start,end],\n",
    "        bbox=bbox,\n",
    "        max_items=500, # for testing, and keep it from hanging\n",
    "        # query={\"eo:cloud_cover\":{\"lt\":20}} #doesn't work\n",
    "    )\n",
    "    print(f\"Search query parameters:\\n{search}\\n\")\n",
    "    results = search.get_all_items_as_dict()\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14e757",
   "metadata": {},
   "source": [
    "Here we run our STAC search and write the results out to a JSON file so we can access it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = query_stac(2020, AOI_bbox, 20, 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', '06-01', '09-30')\n",
    "\n",
    "with open(\"./sample.json\", \"w\") as outfile:\n",
    "    json.dump(search, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b008d",
   "metadata": {},
   "source": [
    "So far, we have not filtered by cloud cover, which is a common filtering parameter for optical imagery. We can use the metadata files included in our STAC search to find the amount of cloud cover in each file and decide if it meets our threshold or not. We will set a cloud cover threshold of 50%. While we are doing this, we will also change the URLs to access the optical imagery from \\\"https://\\\" to AWS S3 URLs (\\\"S3://\\\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_local_data_and_catalog_s3(catalog, bands, save_path, cloud_cover, s3_path=\"s3://\"):\n",
    "    '''Given path to a response json from a sat-api query, make a copy changing urls to local paths'''\n",
    "    creds = maap.aws.earthdata_s3_credentials('https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials')\n",
    "    aws_session = boto3.session.Session(\n",
    "        aws_access_key_id=creds['accessKeyId'], \n",
    "        aws_secret_access_key=creds['secretAccessKey'],\n",
    "        aws_session_token=creds['sessionToken'],\n",
    "        region_name='us-west-2')\n",
    "    s3 = aws_session.client('s3')\n",
    "    \n",
    "    with open(catalog) as f:\n",
    "        clean_features = []\n",
    "        asset_catalog = json.load(f)\n",
    "   \n",
    "        # Remove duplicate scenes\n",
    "        features = asset_catalog['features']\n",
    "        \n",
    "        for feature in features:\n",
    "            umm_json = feature['links'][6]['href']\n",
    "            umm_data = !curl -i {umm_json}\n",
    "            for line in umm_data:\n",
    "                if \"CLOUD_COVERAGE\" in line:\n",
    "                    cc_perc = (int(line.split(\"CLOUD_COVERAGE\")[-1].split('\"')[4]))\n",
    "                    if cc_perc > cloud_cover:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            for band in bands:\n",
    "                                output_file = feature['assets'][band]['href'].replace('https://data.lpdaac.earthdatacloud.nasa.gov/', s3_path)\n",
    "                                bucket_name = output_file.split(\"/\")[2]\n",
    "                                s3_key = \"/\".join(output_file.split(\"/\")[3:])\n",
    "                                head = s3.head_object(Bucket = bucket_name, Key = s3_key, RequestPayer='requester')\n",
    "                                if head['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "                                    feature['assets'][band]['href'] = output_file\n",
    "                            clean_features.append(feature)\n",
    "                        except botocore.exceptions.ClientError as e:\n",
    "                            if e.response['Error']['Code'] == \"404\":\n",
    "                                print(f\"The object does not exist. {output_file}\")\n",
    "                            else:\n",
    "                                raise\n",
    "        # save and updated catalog with local paths\n",
    "        asset_catalog['features'] = clean_features\n",
    "        local_catalog = catalog.replace('sample', 'local-s3')\n",
    "        with open(local_catalog,'w') as jsonfile:\n",
    "            json.dump(asset_catalog, jsonfile)\n",
    "        \n",
    "        return local_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca1a02",
   "metadata": {},
   "source": [
    "When run, this will create a new JSON file that will only include files that meet our cloud cover threshold and have S3 URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd704812",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_cat = write_local_data_and_catalog_s3('./sample.json', ['B02','B03','B04','B05','B06','B07','Fmask'], './', 60, s3_path=\"s3://\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb51fa",
   "metadata": {},
   "source": [
    "Now that we have our images that meet our requirements, we will composite them into a multiband image for our AOI. We will composite the image on a band-by-band basis, so we first have to get a list of all the filepaths for each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBandLists(inJSON, bandnum, tiles=['']):\n",
    "    bands = dict({2:'B02', 3:'B03', 4:'B04', 5:'B05', 6:'B06', 7:'B07',8:'Fmask'})\n",
    "    BandList = []\n",
    "    with open(inJSON) as f:\n",
    "        response = json.load(f)\n",
    "    for i in range(len(response['features'])):\n",
    "        try:\n",
    "            getBand = response['features'][i]['assets'][bands[bandnum]]['href']\n",
    "            # check 's3' is at position [:2]\n",
    "            if getBand.startswith('s3', 0, 2):\n",
    "                BandList.append(getBand)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                \n",
    "    BandList.sort()\n",
    "    return BandList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d08360",
   "metadata": {},
   "source": [
    "We will build a band list of file paths for each image band. We will also access the 'fmask' band to mask out clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_bands = GetBandLists('./local-s3.json', 2)\n",
    "green_bands = GetBandLists('./local-s3.json', 3)\n",
    "red_bands = GetBandLists('./local-s3.json', 4)\n",
    "nir_bands = GetBandLists('./local-s3.json', 5)\n",
    "swir_bands = GetBandLists('./local-s3.json', 6)\n",
    "swir2_bands = GetBandLists('./local-s3.json', 7)\n",
    "fmask_bands = GetBandLists('./local-s3.json', 8)\n",
    "\n",
    "print('number of each images in each band = ', len(blue_bands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169d234",
   "metadata": {},
   "source": [
    "Reading in our HLS data and creating our composite: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8025b",
   "metadata": {},
   "source": [
    "We will not read all of the the HLS data, as we only need what's included in our AOI. To do this \"windowed read\" we need to know the dimensions, in pixels, of the window. To do this we need to convert our AOI to a projected coordinate system (UTM) and calculate the number of columns and rows we will need, using a pixel resolution of 30m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cdeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(bbox, res=30):\n",
    "    left, bottom, right, top = bbox\n",
    "    width = int((right-left)/res)\n",
    "    height = int((top-bottom)/res)\n",
    "\n",
    "    return height,width\n",
    "\n",
    "# convert to m\n",
    "AOI_utm = AOI.to_crs('epsg:32617')\n",
    "height, width = get_shape(AOI_utm.bounds.iloc[0].to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e4f0d",
   "metadata": {},
   "source": [
    "When building a maximum-NDVI composite, the first data we need is the red and NIR bands to make an NDVI band for each image. We will use 'riotiler' to perform a windowed read of our data. We will also read the 'fmask' layer as we can use this to mask out unwanted pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4521da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(file, in_bbox, height, width, epsg=\"epsg:4326\", dst_crs=\"epsg:4326\"):\n",
    "    '''Read a window of data from the raster matching the tile bbox''' \n",
    "    with COGReader(file) as cog:\n",
    "        img = cog.part(in_bbox, bounds_crs=epsg, max_size=None, dst_crs=dst_crs, height=height, width=width)\n",
    "    \n",
    "    return (np.squeeze(img.as_masked().astype(np.float32)) * 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cfdd0",
   "metadata": {},
   "source": [
    "Our AWS session has an expiry time. Now would be a good time to renew our access key to ensure we do not encounter any timeout issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting AWS credentials...')\n",
    "aws_session = get_aws_session_DAAC()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c006e",
   "metadata": {},
   "source": [
    "Using our renewed session, for each band we will read in the relevant band in each of our images, creating an array of image bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de80572",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.Env(aws_session):\n",
    "    print('reading red bands...')\n",
    "    red_stack = np.array([ReadData(red_bands[i], AOI_bbox, height, width, epsg=\"epsg:4326\", dst_crs=\"epsg:4326\") for i in range(len(red_bands))])\n",
    "    print('reading nir bands...')\n",
    "    nir_stack = np.array([ReadData(nir_bands[i], AOI_bbox, height, width, epsg=\"epsg:4326\", dst_crs=\"epsg:4326\") for i in range(len(nir_bands))])\n",
    "    print('reading fmask bands...')\n",
    "    fmask_stack = np.array([ReadData(fmask_bands[i], AOI_bbox, height, width, epsg=\"epsg:4326\", dst_crs=\"epsg:4326\") for i in range(len(fmask_bands))])\n",
    "    print('finished')\n",
    "    \n",
    "print(\"number of red_bands = \", np.shape(red_stack)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e988ee",
   "metadata": {},
   "source": [
    "Now that we have our red band array and NIR band array we can make an NDVI image. While we do this, we will also apply our 'fmask' to remove any pixels that contain cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40014e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_stack =  np.ma.array(np.where(((fmask_stack==1)), -9999, (nir_stack-red_stack)/(nir_stack+red_stack)))\n",
    "ndvi_stack = np.where((~np.isfinite(ndvi_stack)) | (ndvi_stack>1), -9999, ndvi_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c29a07",
   "metadata": {},
   "source": [
    "At this point we can plot our images and see which ones contain cloud coverage. These images have gaps where there is no data or cloud coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdd5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,7, figsize=(33,30))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ndvi_stack[i] = np.where((ndvi_stack[i]>1) | (ndvi_stack[i]<-1), 0, ndvi_stack[i])\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    ax.imshow(ndvi_stack[i], cmap='viridis', clim=(0.1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efe3b2",
   "metadata": {},
   "source": [
    "Now that we have a stack of NDVI bands, we can create an index band that maps the pixel position from each band where the NDVI value is greatest. We can use this to locate the pixels we want to use in our composite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f51837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bool mask where there is no value in any of the NDVI layers\n",
    "print(\"Make NDVI valid mask\")\n",
    "print(\"shape:\\t\\t\", np.ma.array(ndvi_stack).shape)\n",
    "MaxNDVI = np.ma.max(np.ma.array(ndvi_stack),axis=0)\n",
    "BoolMask = np.ma.getmask(MaxNDVI)\n",
    "del MaxNDVI\n",
    "\n",
    "## Get the argmax index positions from the stack of NDVI images\n",
    "print(\"Get stack nan mask\")\n",
    "ndvi_stack = np.ma.array(ndvi_stack)\n",
    "print(\"Calculate Stack max NDVI image\")\n",
    "NDVImax = np.nanargmax(ndvi_stack,axis=0)\n",
    "## create a tmp array (binary mask) of the same input shape\n",
    "NDVItmp = np.ma.zeros(ndvi_stack.shape, dtype=bool)\n",
    "\n",
    "## for each dimension assign the index position (flattens the array to a LUT)\n",
    "print(\"Create LUT of max NDVI positions\")\n",
    "for i in range(np.shape(ndvi_stack)[0]):\n",
    "    NDVItmp[i,:,:]=NDVImax==i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65562a45",
   "metadata": {},
   "source": [
    "Now that we have our NDVI lookup table and a list of all the images for each band, we can make composites based upon the maximum NDVI value. For this we will use the following two functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ee9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CollapseBands(inArr, NDVItmp, BoolMask):\n",
    "    inArr = np.ma.masked_equal(inArr, 0)\n",
    "    inArr[np.logical_not(NDVItmp)]=0 \n",
    "    compImg = np.ma.masked_array(inArr.sum(0), BoolMask)\n",
    "    #print(compImg)\n",
    "    return compImg\n",
    "\n",
    "def CreateComposite(file_list, NDVItmp, BoolMask, in_bbox, height, width, epsg, dst_crs):\n",
    "    MaskedFile = [ReadData(file_list[i], in_bbox, height, width, epsg, dst_crs) for i in range(len(file_list))]\n",
    "    Composite=CollapseBands(MaskedFile, NDVItmp, BoolMask)\n",
    "    return Composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742045e",
   "metadata": {},
   "source": [
    "For each band we will read in all the images (for the required band) and create a composite based on the maximum NDVI value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52018452",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_session = get_aws_session_DAAC() \n",
    "with rio.Env(aws_session):\n",
    "    print('Creating Blue Composite')\n",
    "    blue_comp = CreateComposite(blue_bands, NDVItmp, BoolMask, AOI_bbox, height, width, \"epsg:4326\", \"epsg:4326\")\n",
    "    print('Creating Green Composite')\n",
    "    green_comp = CreateComposite(green_bands, NDVItmp, BoolMask, AOI_bbox, height, width, \"epsg:4326\", \"epsg:4326\")\n",
    "    print('Creating Red Composite')\n",
    "    red_comp = CreateComposite(red_bands, NDVItmp, BoolMask, AOI_bbox, height, width, \"epsg:4326\", \"epsg:4326\")\n",
    "    print('Creating NIR Composite')\n",
    "    nir_comp = CreateComposite(nir_bands, NDVItmp, BoolMask, AOI_bbox, height, width, \"epsg:4326\", \"epsg:4326\")\n",
    "    print('Creating SWIR Composite')\n",
    "    swir_comp = CreateComposite(swir_bands, NDVItmp, BoolMask, AOI_bbox, height, width, \"epsg:4326\", \"epsg:4326\")\n",
    "    print('Creating SWIR2 Composite')\n",
    "    swir2_comp = CreateComposite(swir2_bands, NDVItmp, BoolMask, AOI_bbox, height, width, \"epsg:4326\", \"epsg:4326\")\n",
    "    print('Creating NDVI Composite')\n",
    "    ndvi_comp = CollapseBands(ndvi_stack, NDVItmp, BoolMask)\n",
    "    print('Creating fmask Composite')\n",
    "    fmask_comp = CollapseBands(fmask_stack, NDVItmp, BoolMask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1eeebc",
   "metadata": {},
   "source": [
    "We can look at our NDVI composite image and see we now have a complete image for our AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ac560",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(20,20))\n",
    "ax.axes.xaxis.set_visible(False)\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "plt.imshow(np.where(fmask_comp==1, -9999, ndvi_comp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291022ee",
   "metadata": {},
   "source": [
    "Now that we have a 7-band composite image, we can use these bands to calculate a suite of common vegetation indices using the following functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVI\n",
    "def calcSAVI(red, nir):\n",
    "    savi = ((nir - red)/(nir + red + 0.5))*(1.5)\n",
    "    print('\\tSAVI Created')\n",
    "    return savi\n",
    "\n",
    "# NDMI\n",
    "def calcNDMI(nir, swir):\n",
    "    ndmi = (nir - swir)/(nir + swir)\n",
    "    print('\\tNDMI Created')\n",
    "    return ndmi\n",
    "\n",
    "# EVI\n",
    "def calcEVI(blue, red, nir):\n",
    "    evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1))\n",
    "    print('\\tEVI Created')\n",
    "    return evi\n",
    "\n",
    "# NBR\n",
    "def calcNBR(nir, swir2):\n",
    "    nbr = (nir - swir2)/(nir + swir2)\n",
    "    print('\\tNBR Created')\n",
    "    return nbr\n",
    "\n",
    "# MSAVI\n",
    "def calcMSAVI(red, nir):\n",
    "    msavi = (2 * nir + 1 - np.sqrt((2 * nir + 1)**2 - 8 * (nir - red))) / 2\n",
    "    print('\\tMSAVI Created')\n",
    "    return msavi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fbbc8",
   "metadata": {},
   "source": [
    "We can call these functions and make our additional indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate covars\n",
    "print(\"Generating covariates\")\n",
    "SAVI = calcSAVI(red_comp, nir_comp)\n",
    "#print(\"NDMI\")\n",
    "NDMI = calcNDMI(nir_comp, swir_comp)\n",
    "#print(\"EVI\")\n",
    "EVI = calcEVI(blue_comp, red_comp, nir_comp)\n",
    "#print(\"NBR\")\n",
    "NBR = calcNBR(nir_comp, swir2_comp)\n",
    "MSAVI = calcMSAVI(red_comp, nir_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3bcc7",
   "metadata": {},
   "source": [
    "We have a suite of 12 bands now, that we can merge them together into a single 12-band image stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating raster stack...\\n\")\n",
    "stack = np.transpose([blue_comp, green_comp, red_comp, nir_comp, swir_comp, swir2_comp, ndvi_comp, SAVI, MSAVI, NDMI, EVI, NBR], [0, 1, 2])\n",
    "stack = np.where(fmask_comp==1, -9999, stack)\n",
    "print(np.shape(stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2902c7",
   "metadata": {},
   "source": [
    "Display Results:\n",
    "\n",
    "We can look at each of these bands by using 'matplotlib' to plot each one individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804be413",
   "metadata": {},
   "outputs": [],
   "source": [
    "n: int = len(stack)\n",
    "#topo_cmaps = [\"bone\",\"Spectral\", \"magma\", \"RdBu\", \"coolwarm\"]\n",
    "topo_cmaps = ['Blues','Greens','Reds','PuRd','OrRd','BuPu','YlGn','GnBu','YlOrBr','inferno','plasma', 'viridis']\n",
    "print(stack.shape)\n",
    "bandnames = ['blue_comp', 'green_comp', 'red_comp', 'nir_comp', 'swir_comp', 'swir2_comp', 'ndvi_comp', 'SAVI', 'MSAVI', 'NDMI', 'EVI', 'NBR']\n",
    "\n",
    "font = {'size'   : 30}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "#axs = [[0,0],[0,1],[0,2],[0,3],[0,4],[0,5],[1,0],[1,1],[1,2],[1,3],[1,4],[1,5],[2,0],[2,1],[2,2],[2,3],[2,4],[2,5]]  \n",
    "fig, axes = plt.subplots(3,4, figsize=(33,30))\n",
    "print(axes.flat)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(stack[i], cmap=topo_cmaps[i], clim=(np.percentile(stack[i], 10), np.percentile(stack[i], 90)))\n",
    "    ax.set_title(bandnames[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffa306",
   "metadata": {},
   "source": [
    "We can also visualize our composite NDVI band on our interactive 'folium' map. You can see that even though we found multiple images, by using a windowed read we were able to just read and process the data we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[38.6, -78.5], zoom_start=9, tiles='CartoDB positron')\n",
    "AOI_bx = AOI.bounds\n",
    "#folium.GeoJson(AOI, style_function=lambda x: {'fillColor': 'orange','opacity':0}).add_to(m)\n",
    "geo_r = folium.raster_layers.ImageOverlay(np.ma.getdata(ndvi_comp), opacity=1, bounds=[[AOI_bx['miny'][0],AOI_bx['minx'][0]],[AOI_bx['maxy'][0],AOI_bx['maxx'][0]]])\n",
    "geo_r.add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
